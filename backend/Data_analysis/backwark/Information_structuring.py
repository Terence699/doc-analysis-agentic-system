import re
import json
import os
from typing import List, Dict, Any
from transformers import Qwen2TokenizerFast
import asyncio
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

# âœ… LangChain 1.0 æ–°çš„å¯¼å…¥è·¯å¾„
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_openai import ChatOpenAI  # åœ¨çº¿ LLM
from pydantic import BaseModel, Field

# ==================== åŠ è½½ç¯å¢ƒå˜é‡ ====================
load_dotenv()

# ==================== é…ç½® ====================
TOKENIZER_PATH = os.getenv("QWEN_TOKENIZER_PATH", "/home/data/nongwa/workspace/Data_analysis/Qwen-tokenizer")
tokenizer = Qwen2TokenizerFast.from_pretrained(TOKENIZER_PATH)
CHUNK_SIZE = int(os.getenv("ANALYSIS_CHUNK_SIZE", "1500"))  # tokenæ•°
MAX_WORKERS = int(os.getenv("ANALYSIS_MAX_WORKERS", "10"))  # âœ… å¹¶å‘æ•°

# API é…ç½®ï¼ˆä» .env æ–‡ä»¶è¯»å–ï¼‰
API_KEY = os.getenv("ANALYSIS_API_KEY", "")
API_BASE = os.getenv("ANALYSIS_API_BASE", "https://dashscope.aliyuncs.com/compatible-mode/v1")
MODEL_NAME = os.getenv("ANALYSIS_MODEL_NAME", "qwen3-max")

# ==================== æ•°æ®æ¨¡å‹ ====================
class ExtractedTable(BaseModel):
    """æå–çš„è¡¨æ ¼"""
    title: str = Field(description="è¡¨æ ¼æ ‡é¢˜")
    headers: List[str] = Field(description="è¡¨å¤´åˆ—è¡¨")
    rows: List[List[str]] = Field(description="æ•°æ®è¡Œåˆ—è¡¨")
    note: str = Field(default="", description="å¤‡æ³¨è¯´æ˜")

class ChunkAnalysis(BaseModel):
    """å•ä¸ªchunkçš„åˆ†æç»“æœ"""
    summary: str = Field(description="å†…å®¹æ‘˜è¦")
    tables: List[ExtractedTable] = Field(description="æå–çš„è¡¨æ ¼åˆ—è¡¨")
    key_points: List[str] = Field(description="å…³é”®è¦ç‚¹åˆ—è¡¨")

# ==================== Markdownåˆ‡åˆ†å™¨ ====================
class TitleBasedMarkdownSplitter:
    """åŸºäºæ ‡é¢˜çš„Markdownåˆ‡åˆ†å™¨"""
    
    def __init__(self, chunk_size: int = 500):
        self.chunk_size = chunk_size
        self.head_pattern = re.compile(r"^#+\s")
    
    def split_text(self, markdown: str) -> List[Document]:
        """åˆ‡åˆ†markdownæ–‡æœ¬"""
        lines = markdown.splitlines(keepends=True)
        split_points = self._find_title_split_points(lines)
        initial_chunks = self._create_chunks_by_title(lines, split_points)
        return self._merge_small_chunks(initial_chunks)
    
    def _find_title_split_points(self, lines: List[str]) -> List[Dict]:
        """æ‰¾åˆ°æ‰€æœ‰æ ‡é¢˜åˆ‡åˆ†ç‚¹ï¼ˆæ”¹è¿›ç‰ˆï¼šä¿ç•™å®Œæ•´æ ‡é¢˜è·¯å¾„ï¼‰"""
        points = [{"line": 0, "level": 0, "metadata": {}}]
        
        # âœ… æ”¹ç”¨æ ˆç»“æ„ä¿å­˜æ ‡é¢˜è·¯å¾„
        title_stack = []  # [(level, title), ...]
        
        for i, line in enumerate(lines):
            level = self._get_header_level(line)
            if level > 0:
                title = line.strip("#").strip()
                
                # å¼¹å‡ºæ‰€æœ‰æ›´é«˜æˆ–åŒçº§çš„æ ‡é¢˜
                while title_stack and title_stack[-1][0] >= level:
                    title_stack.pop()
                
                # åŠ å…¥å½“å‰æ ‡é¢˜
                title_stack.append((level, title))
                
                # æ„å»ºå®Œæ•´è·¯å¾„çš„ metadata
                metadata = {}
                for lvl, tit in title_stack:
                    # âœ… æ”¹ä¸º "Header_1", "Header_2" ç­‰ï¼Œé¿å…è¦†ç›–
                    metadata[f"Header_{lvl}"] = tit
                
                # âœ… é¢å¤–ä¿å­˜å®Œæ•´è·¯å¾„
                metadata["header_path"] = " > ".join([t for _, t in title_stack])
                
                points.append({
                    "line": i, 
                    "level": level, 
                    "metadata": metadata.copy()
                })
        
        points.append({"line": len(lines), "level": 0, "metadata": {}})
        return points
    
    def _create_chunks_by_title(self, lines: List[str], points: List[Dict]) -> List[Document]:
        """æŒ‰æ ‡é¢˜åˆ›å»ºåˆå§‹chunks"""
        chunks = []
        for i in range(len(points) - 1):
            start = points[i]["line"]
            end = points[i + 1]["line"]
            content = "".join(lines[start:end])
            metadata = points[i]["metadata"]
            chunks.append(Document(page_content=content, metadata=metadata))
        return chunks
    
    def _merge_small_chunks(self, chunks: List[Document]) -> List[Document]:
        """åˆå¹¶å°å—"""
        result = []
        chunk_tmp = []
        head_chunk_tmp = []
        current_length = 0
        
        for chunk in chunks:
            chunk_tokens = len(tokenizer(chunk.page_content)["input_ids"])
            current_length += chunk_tokens
            
            if current_length <= self.chunk_size:
                chunk_tmp.append(chunk)
                if self.head_pattern.match(chunk.page_content):
                    head_chunk_tmp.append(chunk)
                else:
                    head_chunk_tmp = []
            else:
                if head_chunk_tmp:
                    chunk_tmp = chunk_tmp[:-len(head_chunk_tmp)]
                
                if chunk_tmp:
                    result.append(self._combine_chunks(chunk_tmp))
                
                chunk_tmp = head_chunk_tmp + [chunk]
                head_chunk_tmp = [chunk] if self.head_pattern.match(chunk.page_content) else []
                current_length = len(tokenizer("\n".join([c.page_content for c in chunk_tmp]))["input_ids"])
        
        if chunk_tmp:
            result.append(self._combine_chunks(chunk_tmp))
        
        return result
    
    def _combine_chunks(self, chunks: List[Document]) -> Document:
        """åˆå¹¶å¤šä¸ªchunk"""
        content = "\n".join([c.page_content for c in chunks])
        metadata = {}
        for c in chunks:
            metadata.update(c.metadata)
        return Document(page_content=content, metadata=metadata)
    
    def _get_header_level(self, line: str) -> int:
        """è·å–æ ‡é¢˜çº§åˆ«"""
        match = re.match(r"^(#+)\s+", line)
        return len(match.group(1)) if match else 0

# ==================== æ•°æ®åˆ†æå™¨ ====================
class DataAnalyzer:
    """æ•°æ®åˆ†æå™¨ (è°ƒç”¨åœ¨çº¿ LLM + å¹¶å‘)"""
    
    def __init__(self, api_key: str = API_KEY, base_url: str = API_BASE, model: str = MODEL_NAME, max_workers: int = MAX_WORKERS):
        # åˆå§‹åŒ–åœ¨çº¿ LLM
        self.llm = ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model=model,
            temperature=0.1,
            max_tokens=2048,
            http_client=None,  # æ˜ç¡®è®¾ç½® http_client ä¸º Noneï¼Œé¿å… proxies å†²çª
        )
        
        self.splitter = TitleBasedMarkdownSplitter(chunk_size=CHUNK_SIZE)
        self.output_parser = PydanticOutputParser(pydantic_object=ChunkAnalysis)
        self.max_workers = max_workers
        
        # Prompt
        self.prompt = PromptTemplate(
            template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æ¡£åˆ†æå¸ˆï¼Œæ“…é•¿ä»å¹´åº¦æŠ¥å‘Šã€è´¢åŠ¡æŠ¥è¡¨ç­‰æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–æ•°æ®ã€‚

## åˆ†æä»»åŠ¡
å¯¹ä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µè¿›è¡Œæ·±åº¦åˆ†æï¼Œè¾“å‡ºç»“æ„åŒ–æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰ã€‚

## è¾“å‡ºè¦æ±‚

### 1. å†…å®¹æ‘˜è¦ (summary)
- ç”¨ 1-2 å¥è¯æ¦‚æ‹¬è¯¥ç‰‡æ®µçš„æ ¸å¿ƒå†…å®¹
- çªå‡ºæ•°å­—ã€æ—¶é—´ã€å˜åŒ–è¶‹åŠ¿ç­‰å…³é”®ä¿¡æ¯
- å¦‚æœåŒ…å«å¤šä¸ªä¸»é¢˜ï¼Œç®€è¦åˆ—ä¸¾

### 2. è¡¨æ ¼æå– (tables)
**è½¬æ¢åŸåˆ™**ï¼š
- **åŸæ–‡å·²æœ‰è¡¨æ ¼**ï¼šç›´æ¥æå–å¹¶ä¿ç•™åŸå§‹ç»“æ„
- **å¯ç»“æ„åŒ–æ•°æ®**ï¼šå°†åˆ—è¡¨ã€å¯¹æ¯”ã€ç»Ÿè®¡ç­‰ä¿¡æ¯è½¬ä¸ºè¡¨æ ¼
  - ç¤ºä¾‹ï¼šå¤šä¸ªå¹¶åˆ—æ•°æ®ï¼ˆå¦‚"æ”¶å…¥100ä¸‡ã€æ”¯å‡º80ä¸‡ã€å‡€åˆ©æ¶¦20ä¸‡"ï¼‰
  - ç¤ºä¾‹ï¼šæ—¶é—´åºåˆ—å¯¹æ¯”ï¼ˆå¦‚"2023å¹´Xï¼Œ2024å¹´Y"ï¼‰
  - ç¤ºä¾‹ï¼šå¤šç»´åº¦æè¿°ï¼ˆå¦‚"äº§å“Aç‰¹æ€§1ã€ç‰¹æ€§2ï¼›äº§å“Bç‰¹æ€§1ã€ç‰¹æ€§2"ï¼‰
- **çº¯å™è¿°æ€§æ–‡å­—**ï¼šå¦‚æ”¿ç­–è¯´æ˜ã€é£é™©æç¤ºã€æµç¨‹æè¿°ç­‰ï¼Œä¸å¼ºåˆ¶è½¬è¡¨æ ¼

**è¡¨æ ¼è¦æ±‚**ï¼š
- `title`: ç®€æ´æ˜ç¡®çš„è¡¨æ ¼æ ‡é¢˜ï¼ˆå¦‚"2024å¹´æ”¶æ”¯å¯¹æ¯”"ï¼‰
- `headers`: åˆ—åæ¸…æ™°ï¼ˆå¦‚["é¡¹ç›®", "é‡‘é¢", "å æ¯”"]ï¼‰
- `rows`: æ¯è¡Œæ•°æ®å¯¹é½
- `note`: è¡¥å……è¯´æ˜ï¼ˆå¦‚å•ä½ã€æ•°æ®æ¥æºã€è®¡ç®—æ–¹æ³•ï¼‰

**è¡¨æ ¼æ•°é‡**ï¼š
- æ ¹æ®å†…å®¹è‡ªç„¶åˆ’åˆ†ï¼Œä¸å¼ºåˆ¶æ•°é‡
- ä¼˜å…ˆåˆå¹¶ç›¸å…³æ•°æ®åˆ°åŒä¸€è¡¨æ ¼
- é¿å…è¿‡åº¦æ‹†åˆ†ï¼ˆå¦‚å°†5ä¸ªæ•°å­—æ‹†æˆ5ä¸ªè¡¨æ ¼ï¼‰

### 3. å…³é”®è¦ç‚¹ (key_points)
- æå– 3-10 ä¸ªæ ¸å¿ƒè¦ç‚¹ï¼ˆè§†å†…å®¹è€Œå®šï¼‰
- æ¯ä¸ªè¦ç‚¹ç‹¬ç«‹æˆå¥ï¼ŒåŒ…å«å®Œæ•´ä¿¡æ¯
- é‡ç‚¹æ ‡æ³¨ï¼š
  - æ•°å­—å˜åŒ–ï¼ˆå¦‚"è¥æ”¶åŒæ¯”å¢é•¿25%"ï¼‰
  - æ—¶é—´èŠ‚ç‚¹ï¼ˆå¦‚"2024å¹´7æœˆå®æ–½æ–°æ”¿ç­–"ï¼‰
  - å…³é”®ç»“è®ºï¼ˆå¦‚"ä¸»è¦é£é™©ä¸ºå¸‚åœºæ³¢åŠ¨"ï¼‰
- è¦ç‚¹ä¹‹é—´é¿å…é‡å¤ä¿¡æ¯

---

## è¾“å…¥æ•°æ®

**æ–‡æ¡£ç‰‡æ®µ**ï¼š
```
{markdown_chunk}
```

**æ–‡æ¡£å±‚çº§å…ƒæ•°æ®**ï¼š
```json
{metadata}
```
- `header_path`: å®Œæ•´æ ‡é¢˜è·¯å¾„ï¼ˆå¦‚"Â§3 è´¢åŠ¡æŒ‡æ ‡ > 3.1 ä¸»è¦æ•°æ®"ï¼‰
- `Header_1`, `Header_2`ç­‰ï¼šå„çº§æ ‡é¢˜å†…å®¹

---

## è¾“å‡ºæ ¼å¼

{format_instructions}

---

## æ³¨æ„äº‹é¡¹
1. **ä¿æŒå®¢è§‚**ï¼šä¸æ·»åŠ åŸæ–‡æ²¡æœ‰çš„è§£è¯»æˆ–æ¨æµ‹
2. **æ•°æ®å‡†ç¡®**ï¼šç¡®ä¿æ•°å­—ã€ç™¾åˆ†æ¯”ã€å•ä½æ­£ç¡®
3. **ä¿¡æ¯å®Œæ•´**ï¼šå…³é”®ä¸Šä¸‹æ–‡ï¼ˆå¦‚æ—¶é—´èŒƒå›´ã€å¯¹æ¯”åŸºå‡†ï¼‰ä¸èƒ½ä¸¢å¤±
4. **æ ¼å¼ç»Ÿä¸€**ï¼š
   - é‡‘é¢åŠ åƒåˆ†ä½ï¼ˆå¦‚"1,234,567.89"ï¼‰
   - ç™¾åˆ†æ¯”ä¿ç•™å°æ•°ï¼ˆå¦‚"12.34%"ï¼‰
   - æ—¥æœŸç»Ÿä¸€æ ¼å¼ï¼ˆå¦‚"2024-12-31"ï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š""",
            input_variables=["markdown_chunk", "metadata"],
            partial_variables={"format_instructions": self.output_parser.get_format_instructions()}
        )
        
        self.chain = self.prompt | self.llm
    
    def _process_single_chunk(self, chunk: Document, chunk_id: int) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªchunk (ç”¨äºå¹¶å‘)"""
        try:
            print(f"å¤„ç†å— {chunk_id + 1}...")
            
            # ä¼ å…¥ä¼˜åŒ–åçš„ metadataï¼ˆJSONæ ¼å¼æ›´æ˜“LLMç†è§£ï¼‰
            result = self.chain.invoke({
                "markdown_chunk": chunk.page_content,
                "metadata": json.dumps(chunk.metadata, ensure_ascii=False, indent=2)
            })
            
            analysis = self.output_parser.parse(result.content)
            
            print(f"å— {chunk_id + 1} åˆ†æå®Œæˆ")
            
            return {
                "chunk_id": chunk_id,
                "original_content": chunk.page_content,
                "metadata": chunk.metadata,  # âœ… ä¿ç•™å®Œæ•´ metadata
                "analysis": analysis.model_dump()
            }
        
        except Exception as e:
            print(f"âš ï¸ å— {chunk_id + 1} è§£æå¤±è´¥: {e}")
            return {
                "chunk_id": chunk_id,
                "error": str(e),
                "raw_output": result.content if 'result' in locals() else None
            }
    
    def analyze_ocr_json(self, ocr_json: Dict[str, Any], use_concurrent: bool = True) -> Dict[str, Any]:
        """åˆ†æOCRè¿”å›çš„JSON
        
        Args:
            ocr_json: OCRç»“æœ
            use_concurrent: æ˜¯å¦ä½¿ç”¨å¹¶å‘ (é»˜è®¤True)
        """
        markdown = ocr_json.get("markdown", "")
        
        # 1. åˆ‡åˆ†markdown
        chunks = self.splitter.split_text(markdown)
        print(f"ğŸ“Š æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—")
        
        # 2. åˆ†ææ¯ä¸ªchunk
        if use_concurrent:
            print(f"âš¡ ä½¿ç”¨å¹¶å‘æ¨¡å¼ (max_workers={self.max_workers})")
            analyzed_chunks = self._analyze_concurrent(chunks)
        else:
            print(f"ğŸŒ ä½¿ç”¨ä¸²è¡Œæ¨¡å¼")
            analyzed_chunks = self._analyze_sequential(chunks)
        
        # 3. æ„å»ºæœ€ç»ˆç»“æœ
        return {
            "source": ocr_json,
            "total_chunks": len(chunks),
            "analyzed_chunks": analyzed_chunks,
            "metadata": {
                "chunk_size": CHUNK_SIZE,
                "tokenizer": "Qwen2",
                "model": MODEL_NAME,
                "concurrent": use_concurrent,
                "max_workers": self.max_workers if use_concurrent else 1
            }
        }
    
    def _analyze_sequential(self, chunks: List[Document]) -> List[Dict[str, Any]]:
        """ä¸²è¡Œå¤„ç†"""
        results = []
        for i, chunk in enumerate(chunks):
            result = self._process_single_chunk(chunk, i)
            results.append(result)
        return results
    
    def _analyze_concurrent(self, chunks: List[Document]) -> List[Dict[str, Any]]:
        """å¹¶å‘å¤„ç† (ä½¿ç”¨çº¿ç¨‹æ± )"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = [
                executor.submit(self._process_single_chunk, chunk, i)
                for i, chunk in enumerate(chunks)
            ]
            
            # æŒ‰é¡ºåºæ”¶é›†ç»“æœ
            results = []
            for future in futures:
                result = future.result()
                results.append(result)
            
            # æŒ‰ chunk_id æ’åº
            results.sort(key=lambda x: x["chunk_id"])
            
        return results

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================
if __name__ == "__main__":
    import time
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = DataAnalyzer(
        api_key=API_KEY,
        base_url=API_BASE,
        model=MODEL_NAME,
        max_workers=5  # âœ… å¹¶å‘æ•°
    )
    
    # è¯»å–OCRç»“æœ
    with open("/home/data/nongwa/workspace/data/10åå¤æ”¶å…¥æ··åˆå‹è¯åˆ¸æŠ•èµ„åŸºé‡‘2024å¹´å¹´åº¦æŠ¥å‘Š.json", encoding="utf-8") as f:
        ocr_data = json.load(f)
    
    # æ‰§è¡Œåˆ†æ (å¹¶å‘æ¨¡å¼)
    print("ğŸš€ å¼€å§‹åˆ†æ (å¹¶å‘æ¨¡å¼)...")
    start_time = time.time()
    
    result = analyzer.analyze_ocr_json(ocr_data, use_concurrent=True)
    
    elapsed = time.time() - start_time
    
    # ä¿å­˜ç»“æœ
    import os
    with open(os.path.join(os.path.dirname(__file__),"analyzed_result.json"), "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… åˆ†æå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ° analyzed_result.json")
    print(f"ğŸ“Š æ€»å—æ•°: {result['total_chunks']}")
    print(f"âœ… æˆåŠŸ: {sum(1 for c in result['analyzed_chunks'] if 'analysis' in c)}")
    print(f"âš ï¸ å¤±è´¥: {sum(1 for c in result['analyzed_chunks'] if 'error' in c)}")
    print(f"â±ï¸  è€—æ—¶: {elapsed:.2f}ç§’")
    print(f"âš¡ å¹³å‡æ¯å—: {elapsed/result['total_chunks']:.2f}ç§’")