#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSeek OCR API Server (vLLM) - æç®€ç‰ˆ
åªè¿”å› Markdown å†…å®¹
"""
import os
import io
import re
import argparse
from io import BytesIO
from typing import List

import torch
from PIL import Image

try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None

from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

from vllm import LLM, SamplingParams
from vllm.model_executor.models.registry import ModelRegistry
from deepseek_ocr import DeepseekOCRForCausalLM
from process.ngram_norepeat import NoRepeatNGramLogitsProcessor
from process.image_process import DeepseekOCRProcessor

# -----------------------
# FastAPI App
# -----------------------
app = FastAPI(title="DeepSeek OCR API (vLLM) - Simple", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"], 
    allow_headers=["*"],
)

# -----------------------
# å…¨å±€å˜é‡
# -----------------------
llm = None

# å›ºå®š Prompt
PROMPT_OCR = "<image>\n<|grounding|>Convert the document to markdown."
PROMPT_DESC = "<image>\nDescribe this image in detail."

# -----------------------
# æ¨¡å—çº§ Monkey Patch
# -----------------------
_original_tokenize = DeepseekOCRProcessor.tokenize_with_images

def _patched_tokenize(self, images, bos=True, eos=True, cropping=True, prompt=None):
    if prompt is not None:
        import config
        old = config.PROMPT
        config.PROMPT = prompt
        try:
            return _original_tokenize(self, images, bos, eos, cropping)
        finally:
            config.PROMPT = old
    return _original_tokenize(self, images, bos, eos, cropping)

DeepseekOCRProcessor.tokenize_with_images = _patched_tokenize

# -----------------------
# å·¥å…·å‡½æ•°
# -----------------------
def pdf_to_images(pdf_bytes: bytes, dpi: int = 144) -> List[Image.Image]:
    """PDF è½¬å›¾ç‰‡"""
    if fitz is None:
        raise RuntimeError("æœªå®‰è£… PyMuPDF,è¯·æ‰§è¡Œ: pip install PyMuPDF")
    
    images = []
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    matrix = fitz.Matrix(dpi / 72.0, dpi / 72.0)
    
    for page in doc:
        pix = page.get_pixmap(matrix=matrix, alpha=False)
        img = Image.open(io.BytesIO(pix.tobytes("png")))
        
        if img.mode != "RGB":
            if img.mode in ("RGBA", "LA"):
                bg = Image.new("RGB", img.size, (255, 255, 255))
                bg.paste(img, mask=img.split()[-1])
                img = bg
            else:
                img = img.convert("RGB")
        
        images.append(img)
    
    doc.close()
    return images


def clear_vllm_cache():
    """æ¸…ç† vLLM ç¼“å­˜"""
    if llm is None:
        return
    try:
        if hasattr(llm.llm_engine, 'input_preprocessor'):
            prep = llm.llm_engine.input_preprocessor
            if hasattr(prep, '_mm_processor_cache'):
                prep._mm_processor_cache.clear()
    except:
        pass


def vllm_generate(image: Image.Image, prompt: str) -> str:
    """vLLM æ¨ç†"""
    clear_vllm_cache()
    
    processor = DeepseekOCRProcessor()
    tokenized = processor.tokenize_with_images(images=[image], prompt=prompt)
    
    batch_inputs = [{
        "prompt": prompt,
        "multi_modal_data": {"image": tokenized}
    }]
    
    
    if prompt == PROMPT_OCR:
        logits_proc = [NoRepeatNGramLogitsProcessor(20, 50, {128821, 128822})]
        params = SamplingParams(
            temperature=0.0,
            max_tokens=4096,
            skip_special_tokens=False,
            logits_processors=logits_proc,
            repetition_penalty=1.05,
        )
    else:
        params = SamplingParams(
            temperature=0.0,
            max_tokens=512,
            skip_special_tokens=False,
        )
    
    outputs = llm.generate(batch_inputs, params)
    return outputs[0].outputs[0].text


def clean_markdown(text: str) -> str:
    """æ¸…ç† Markdown (ç§»é™¤ç‰¹æ®Šæ ‡è®°)"""
    # ç§»é™¤ <|ref|> <|det|> ç­‰æ ‡è®°
    text = re.sub(r'<\|ref\|>.*?<\|/ref\|>', '', text)
    text = re.sub(r'<\|det\|>.*?<\|/det\|>', '', text)
    text = re.sub(r'<\|.*?\|>', '', text)
    text = re.sub(r'\[\[.*?\]\]', '', text)
    
    # ç§»é™¤é•¿åˆ†éš”çº¿
    text = re.sub(r'={50,}.*?={50,}', '', text, flags=re.DOTALL)
    
    # è§„èŒƒåŒ–ç©ºç™½
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()


def generate_image_description(image: Image.Image) -> str:
    """ç”Ÿæˆå›¾ç‰‡æè¿°"""
    try:
        result = vllm_generate(image, PROMPT_DESC)
        
        # æ¸…ç†ç‰¹æ®Šæ ‡è®°
        desc = re.sub(r'<\|ref\|>.*?<\|/ref\|>', '', result)
        desc = re.sub(r'<\|det\|>.*?<\|/det\|>', '', desc)
        desc = re.sub(r'<\|.*?\|>', '', desc)
        desc = re.sub(r'\[\[.*?\]\]', '', desc)
        desc = re.sub(r'\s+', ' ', desc).strip()
        
        # æˆªæ–­åˆ°200å­—ç¬¦
        if len(desc) > 200:
            cutoff = desc[:200].rfind('.')
            if cutoff > 100:
                desc = desc[:cutoff + 1]
            else:
                desc = desc[:200].rsplit(' ', 1)[0] + '...'
        
        return desc
    except Exception as e:
        print(f"âš ï¸ å›¾ç‰‡æè¿°å¤±è´¥: {e}")
        return ""


# -----------------------
# æ¨¡å‹åˆå§‹åŒ–
# -----------------------
def initialize_model(model_path: str, gpu_id: int = 0):
    global llm
    
    ModelRegistry.register_model("DeepseekOCRForCausalLM", DeepseekOCRForCausalLM)
    
    if torch.cuda.is_available():
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    
    os.environ['VLLM_USE_V1'] = '0'
    
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
    
    llm = LLM(
        model=model_path,
        hf_overrides={"architectures": ["DeepseekOCRForCausalLM"]},
        block_size=256,
        enforce_eager=False,
        trust_remote_code=True,
        max_model_len=8192,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.9,
        max_num_seqs=100,
        disable_mm_preprocessor_cache=True,
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")


# -----------------------
# API è·¯ç”±
# -----------------------
@app.get("/")
async def root():
    return {
        "service": "DeepSeek OCR (vLLM) - Simple",
        "version": "1.0.0",
        "status": "running"
    }


@app.get("/health")
async def health():
    return {"status": "healthy", "model_ready": llm is not None}


@app.post("/ocr")
async def ocr(
    file: UploadFile = File(...),
    enable_description: bool = Form(False),
):
    """
    OCR æ¥å£ (å›¾ç‰‡æˆ– PDF)
    
    å‚æ•°:
        file: å›¾ç‰‡æ–‡ä»¶ (jpg/png) æˆ– PDF æ–‡ä»¶
        enable_description: æ˜¯å¦ç”Ÿæˆå›¾ç‰‡æè¿°
    
    è¿”å›:
        {
            "markdown": "...",  # Markdown å†…å®¹
            "page_count": 1     # é¡µæ•°
        }
    """
    if llm is None:
        raise HTTPException(503, "æ¨¡å‹æœªåŠ è½½")
    
    try:
        contents = await file.read()
        
        # åˆ¤æ–­æ–‡ä»¶ç±»å‹
        if file.filename.lower().endswith('.pdf'):
            images = pdf_to_images(contents)
        else:
            images = [Image.open(BytesIO(contents)).convert("RGB")]
        
        print(f"å¤„ç† {len(images)} é¡µ...")
        
        # å¤„ç†æ¯ä¸€é¡µ
        md_parts = []
        for idx, img in enumerate(images):
            print(f"   é¡µ {idx + 1}/{len(images)}")
            
            # OCR
            raw = vllm_generate(img, PROMPT_OCR)
            
            # å¦‚æœå¯ç”¨å›¾ç‰‡æè¿°,æ›¿æ¢å›¾ç‰‡æ ‡è®°
            if enable_description:
                # æŸ¥æ‰¾æ‰€æœ‰ <|ref|>image<|/ref|> æ ‡è®°
                img_pattern = r'<\|ref\|>image<\|/ref\|><\|det\|>\[\[.*?\]\]<\|/det\|>'
                
                def replace_with_desc(match):
                    # æå– bbox
                    det_match = re.search(r'\[\[(.*?)\]\]', match.group(0))
                    if det_match:
                        # ç”Ÿæˆæè¿°
                        desc = generate_image_description(img)
                        return f"[å›¾ç‰‡: {desc}]" if desc else "[å›¾ç‰‡]"
                    return "[å›¾ç‰‡]"
                
                raw = re.sub(img_pattern, replace_with_desc, raw)
            
            # æ¸…ç†å¹¶æ·»åŠ 
            cleaned = clean_markdown(raw)
            if cleaned:
                md_parts.append(cleaned)
        
        # åˆå¹¶æ‰€æœ‰é¡µ
        final_md = "\n\n".join(md_parts)
        
        return JSONResponse({
            "markdown": final_md,
            "page_count": len(images)
        })
    
    except Exception as e:
        import traceback
        raise HTTPException(500, f"å¤„ç†å¤±è´¥: {e}\n{traceback.format_exc()}")


# -----------------------
# å¯åŠ¨
# -----------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--gpu-id", type=int, default=0, help="GPU ID")
    parser.add_argument("--port", type=int, default=8707, help="ç«¯å£")
    parser.add_argument("--host", default="0.0.0.0", help="ç›‘å¬åœ°å€")
    
    args = parser.parse_args()
    
    initialize_model(args.model_path, args.gpu_id)
    
    print(f"\nğŸš€ æœåŠ¡å¯åŠ¨: http://{args.host}:{args.port}")
    print(f"ğŸ“– æ¥å£æ–‡æ¡£: http://{args.host}:{args.port}/docs\n")
    
    uvicorn.run(app, host=args.host, port=args.port, workers=1)


if __name__ == "__main__":
    main()